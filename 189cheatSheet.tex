\documentclass[3pt,landscape]{article}
%ss[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{color,graphicx,overpic}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage[shortlabels]{enumitem}


\pdfinfo{
/Title (final.pdf)
/Creator (TeX)
/Producer (pdfTeX 1.40.0)
/Author (Rishi Sharma)
/Subject (Machine Learning)
/Keywords (pdflatex, latex,pdftex,tex)}

% This sets page margins to .5 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.3in,left=.3in,right=.3in,bottom=.3in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                            {-1ex plus -.5ex minus -.2ex}%
                            {0.5ex plus .2ex}%x
                            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                            {-1explus -.5ex minus -.2ex}%
                            {0.5ex plus .2ex}%
                            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                            {-1ex plus -.5ex minus -.2ex}%
                            {1ex plus .2ex}%
                            {\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}
% -----------------------------------------------------------------------

\def\ci{\perp\!\!\!\perp}

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\begin{center}
    \Large{\underline{CS 189 Final Note Sheet}} \\
\end{center}

%Maximum likelihood estimation
%CHECK - Decision theory
%CHECK - Multivariate Gaussians
%CHECK - Linear/quadratic discriminant analysis
%CHECK - Linear regression
%CHECK - Logistic regression
%CHECK - Loss Functions (hinge loss, log loss, misclassification loss)
%CHECK - Optimization (Gradient descent, Newton's Method, Lagrange multipliers)
%CHECK - SVMs (primal, dual)
%Kernels
%CHECK - Nearest neighbour

\subsection*{Bayesian Decision Theory}
Bayes Rule: \(P(\omega|x)=\frac{P(x|\omega)P(\omega)}{P(x)}, P(x)=\sum_{i}P(x|\omega_i)P(\omega_i)\)
\(P(x,w) = P(x|w)P(w) = P(w|x)P(x)\)
\(P(error)=\int_{-\infty}^\infty P(error|x)P(x)dx\)
\(P(error|x)=
    \left\{
    \begin{array}{lr}
        P(\omega_1|x) \text{\ if we decide\ } \omega_2\\
        P(\omega_2|x) \text{\ if we decide\ } \omega_1
    \end{array}
    \right.\)\\
\(\text{0-1 Loss:\ } \lambda(\alpha_i|\omega_j)=
    \left\{
    \begin{array}{lr}
        0 \ \ i=j \text{\ (correct)\ }\\
        1 \ \ i\not=j\text{\ (mismatch)\ }
    \end{array}
    \right.\)\\
\(\text{Expected Loss (Risk):\ } R(\alpha_i|x)=\sum_{j=1}^c\lambda(\alpha_i|\omega_j)P(\omega_j|x)\)\\
\(\text{0-1 Risk:\ } R(\alpha_i|x)=\sum_{j\not=i}^cP(\omega_j|x)=1-P(\omega_i|x)\)\\
%Add decision boundary?, optimal class? Guassian same Variance intuition...

\subsection*{Probabilistic Motivation for Least Squares}
\(y^{(i)} = \theta^\intercal x^{(i)} + \epsilon^{(i)}\ \text{with noise} \ \epsilon{(i)} \sim \mathcal{N}(0,\sigma^2)\)\\
Note: The intercept term $x_0=1$ is accounted for in $\theta$
\(\implies p(y^{(i)}|x^{(i)};\theta) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y^{(i)} - \theta^\intercal x^{(i)})^2}{2\sigma^2}\right)\)
\(\implies L(\theta) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y^{(i)} - \theta^\intercal x^{(i)})^2}{2\sigma^2}\right)\)
\(\implies l(\theta) = m\log\frac{1}{\sqrt{2\pi\sigma^2}} - \frac{1}{2\sigma^2} \sum_{i=1}^m (y^{(i)} - \theta^\intercal x^{(i)})^2 \)
\(\implies \max_\theta l(\theta) \equiv \min_\theta \sum_{i=1}^m (y^{(i)} - h_\theta(x))^2 \)\\
 Gaussian noise in our data set $\{x^{(i)},y^{(i)}\}_{i=1}^m$gives us least squares \\
\(min_\theta ||X\theta - y||_2^2 \equiv \min_\theta \theta^\intercal X^\intercal X \theta - 2 \theta^\intercal X^\intercal y + y^\intercal Y\)
\(\nabla_\theta l(\theta) =  X^\intercal X\theta - X^\intercal y = 0 \implies \boxed{\theta^* = (X^\intercal X)^{-1}X^\intercal y} \)\\
Gradient Descent: $\theta_{t+1} = \theta_t + \alpha (y_t^{(i)} - h(x_t^{(i)}))x_t^{(i)},\ \ h_\theta(x)=\theta^\intercal x$
\subsection*{Least Squares Solution}
\(\min_x ||Ax - y||_2^2 \implies x^* = A^\dagger y \text{\ min norm sol'n}\)\\
Sol'n set: \(x_0 + N(A) = x^* + N(A)\)\[A^\dagger = \left\{\begin{array}{lr}
            (A^\intercal A)^{-1}A^\intercal \text{ \ $A$ full column rank}\\
           A^\intercal (AA^\intercal)^{-1} \text{ \ $A$ full row rank}\\
            V\Sigma^\dagger U^\intercal \text{ \ \ \ \ \ \ \ any $A$} 
    \end{array}\right. \]
L2 Reg: $\min_x ||Ax-y||_2^2 + \lambda||x||_2^2 \implies x^*=(A^TA+\lambda I)^{-1}X^Ty$\\
The above variant is used when A contains a null space. L2 Reg falls out of the MLE when we add a Gaussian prior on x with $\Sigma=cI$. We get L1 Reg when x has a Laplace prior.
%1l-sparcity, regularization prevents overfitting.

\subsection*{Logistic Regression}
Classify $y \in \{0,1\} \implies$Model $p(y=1|x) = \frac{1}{1+e^{-\theta^T x}}= h_\theta(x)$\\
$\frac{dh_\theta}{d\theta} = (\frac{1}{1 + e^{\theta^Tx}})^2 e^{-\theta^Tx} = \frac{1}{1 + e^{\theta^Tx}}\left(1-\frac{1}{1+e^{-\theta^Tx}}\right) = h_\theta(1-h_\theta)$
\(p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} \implies\)
\(L(\theta)  = \prod_{i=1}^m (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \implies \)
\(l(\theta) = \sum_{i=1}^m y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})) \implies\)
\(\nabla_\theta l = \sum_i (y^{(i)} - h_\theta(x^{(i)}))x^{(i)} = X^\intercal (y-h_\theta(X))\),  (want $\max \ l(\theta)$)
Stoch: $\boxed{\theta_{t+1} = \theta_t + \alpha(y^{(j)}_t - h_\theta(x^{(j)}_t))x^{(j)}_t}$\\
Batch: $\boxed{\theta_{t+1} = \theta_t + \alpha X^\intercal(y-h_\theta(X))}$
%Gaussians with equal variance go to logistic, so do poisson and other
%Model log odds as a linear function of input, log(p/(1-p))=B^T x where p=P(y=1|x)
%Neural Net, output layer only, 1 node, sigmoidal with cross entropy loss = logistic regression
%MLE of variance for guassian is biased (mean is unbiased)

\subsection*{Multivariate Gaussian $X \sim \mathcal{N}(\mu,\Sigma)$}
\(f(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)\right)\)
\(\Sigma=E[(X-\mu)(X-\mu)^T]=E[XX^T]-\mu\mu^T\)
\(\Sigma \text{ is PSD} \implies x^T\Sigma x \ge 0 \text{, if inverse exists }\Sigma \text{ must be PD}\)
\(\text{If } X\sim N(\mu,\Sigma),\ \text{then}\ AX+b\sim N(A\mu+b,A\Sigma A^T)\)\\
\(\implies \Sigma^{-\frac{1}{2}}(X-\mu)\sim N(0,I), \text{ where } \Sigma^{-\frac{1}{2}}=U\Lambda^{-\frac{1}{2}}\)\\
\rule{1\linewidth}{0.1pt}\\
%Rotation,Scaling, Shift example
%EVD
The distribution is the result of a linear transformation of a vector of univariate Gaussians $Z \sim \mathcal{N}(0,I)$ such that $X = AZ + \mu$ where we have $\Sigma = AA^\intercal$. From the pdf, we see that the level curves of the distribution decrease proportionally with $x^\intercal\Sigma^{-1}x$ (assume $\mu = 0$) $\implies$ \[\text{$c$-level set of $f$} \propto \{x:x^\intercal\Sigma^{-1}x = c\}\]
\[x^\intercal\Sigma^{-1} = c \equiv x^\intercal U\Lambda^{-1}U^\intercal x = c \implies\]
\[\underbrace{\lambda_1^{-1}(u_1^\intercal x)^2}_\text{axis length: $\sqrt{\lambda_1}$} + \cdots +\underbrace{\lambda_n^{-1}(u_n^\intercal x)^2}_\text{axis length: $\sqrt{\lambda_n}$} = c\]\\
Thus the level curves form an ellipsoid with axis lengths equal to the square root of the eigenvalues of the covariance matrix.

\subsection*{LDA and QDA}
Classify $y\in\{0,1\},$ Model $p(y)=\phi^y\phi^{1-y}$ and $p(x|y=1;\mu_1)=\frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1)\right)$\\
$l(\theta,\mu_0,\mu_1,\Sigma)=log\ \Pi_{i=1}^m p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\Sigma)p(y^{(i)};\Phi)$
gives us $\phi_{MLE}=\frac{1}{m}\sum_{i=1}^m 1\{y^{(i)}=1\}, \mu_{k_{MLE}}=$avg of $x^{(i)}$ classified as k,
$\Sigma_{MLE} =\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu_{y_{(i)}})(x^{(i)}-\mu_{y_{(i)}})^T$.\\
Notice the covariance matrix is the same for all classes in LDA.\\
If $p(x|y)$ multivariate gaussian (w/ shared $\Sigma)$, then $p(y|x)$ is logistic function. The converse is NOT true. LDA makes stronger assumptions about data than does logistic regression.
$h(x)=arg\max_{k}-\frac{1}{2}(x-\mu_k)^T \Sigma^{-1}(x-\mu_k)+log(\pi_k)$ \\where $\pi_k=p(y=k)$\\
\rule{1\linewidth}{0.1pt}\\
For QDA, the model is the same as LDA except that each class has a unique covariance matrix. 
$h(x)=arg\max_{k}-\frac{1}{2}log|\Sigma_k|-\frac{1}{2}(x-\mu_k)^T \Sigma_k^{-1}(x-\mu_k)+log(\pi_k)$

\subsection*{Optimization}
Newton's Method: $\theta_{t+1}=\theta_t-[\nabla_\theta^2f(\theta_t)]^{-1}\nabla_\theta f(\theta_t)$\\
Gradient Decent: $\theta_{t+1}=\theta_t-\alpha\nabla_\theta f(\theta_t)$, for minimizing\\
Lagrange Multipliers:\\
Given\ \ $\min_x f(x)\ s.t.\ g_i(x)=0,\ h_i(x)\le0$\ \ the corresponding Lagrangian is: $L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_ig_i(x)+\sum_{i=1}^l\beta_ih_i(x)$\\
We min over x and max over the Lagrange multipliers $\alpha$ and $\beta$

\subsection*{Support Vector Machines}
In the strictly separable case, the goal is to find a separating hyperplane (like logistic regression) except now we don't just want any hyperplane, but one with the largest margin. \\
$H=\{\omega^Tx+b=0\}$, since scaling $\omega$ and b in opposite directions doesn't change the hyperplane our optimization function should have scaling invariance built into it. Thus, we do it now and define the closest points to the hyperplane $x_{sv}$ (support vectors) to satisfy: $|\omega^Tx_{sv}+b|=1$. The distance from any support vector to the hyper plane is now: $\frac{1}{||\omega||_2}$. Maximizing the distance to the hyperplane is the same as minimizing $||\omega||_2$.\\
The final optimization problem is: $\boxed{\min_{\omega,b} \frac{1}{2}||\omega||_2\ s.t.\ y^{(i)}(w^Tx^{(i)}+b)\ge1, i=1,\dots,m}$\\
Primal: $L_p(\omega,b,\alpha)=\frac{1}{2}||\omega||_2-\sum_{i=1}^m\alpha_i(y^{(i)}(w^Tx^{(i)}+b)-1)$\\
$\frac{\partial L_p}{\partial\omega}=\omega-\sum \alpha_iy^{(i)}x^{(i)}=0 \implies \omega=\sum \alpha_iy^{(i)}x^{(i)}$\\
$\frac{\partial L_p}{\partial b}=-\sum \alpha_iy^{(i)}=0, \text{\ \ \ Note: }\alpha_i\ne 0$ only for support vectors.\\
Substitute the derivatives into the primal to get the dual.\\
Dual: $L_d(\alpha)=\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^my^{(i)}y^{(j)}\alpha_i\alpha_j(x^{(i)})^Tx^{(j)}$\\
KKT says $\alpha_n (y_n (w^T x_n + b) - 1) = 0$ where $\alpha_n > 0$.
\rule{1\linewidth}{0.1pt}\\
In the non-separable case we allow points to cross the marginal boundary by some amount $\xi$ and penalize it.\\
$\boxed{\min_{\omega,b} \frac{1}{2}||\omega||_2+C\sum_{i=1}^m\xi_i\ \ s.t.\ \ y^{(i)}(w^Tx^{(i)}+b)\ge1-\xi_i}$\\
The dual for non-separable doesn't change much except that each $\alpha_i$ now has an upper bound of C $\implies 0\le\alpha_i\le C$
%Kernel stuff?

\subsection*{Loss Functions}
In general the loss function consists of two parts, the loss term and the regularization term. $J(\omega)=\sum_i Loss_i + \lambda R(\omega)$

%Loss Functions (hinge loss, log loss, misclassification loss)

\subsection*{Nearest Neighbor}
Key Idea: Store all training examples $<x_i,f(x_i)>$\\
{\bf NN}: Find closest training point using some distance metric and take its label.\\
{\bf k-NN}: Find closest k training points and take on the most likely label based on some voting scheme (mean, median,...)\\
{\bf Behavior at the limit}: 1NN $lim_{N\to\infty}\ \epsilon^*\le\epsilon_{NN}\le 2\epsilon^*$
$\epsilon^*=\text{error of optimal prediction},\ \epsilon_{nn}=\text{error of 1NN classifier}$\\
KNN \ $lim_{N\to\infty,K\to\infty}, \frac{K}{N}\to 0, \epsilon_{knn}=\epsilon^*$\\
{\bf Curse of dimentionality}: As the number of dimensions increases, everything becomes farther appart. Our low dimension intuition falls apart. Consider the Hypersphere/Hypercube ratio, it's close to zero at d=10.
How do deal with this curse:\\
\ \ 1. Get more data to fill all of that empty space\\
\ \ 2. Get better features, reducing the dimentionality and packing the data closer together. Ex: Bag-of-words, Histograms,...\\
\ \ 3. Use a better distance metric.\\
Minkowski: $Dis_p(x,y)=(\sum_{i=1}^d|x_i-y_u|^p)^{\frac{1}{p}}=||x-y||_p$\\
0-norm: $Dis_0(x,y)=\sum_{i=1}^d I|x_i=y_i|$\\
Mahalanobis: $Dis_M(x,y|\Sigma)=\sqrt{(x-y)^T\Sigma^{-1}(x-y)}$\\
In high-d we get ``Hubs" s.t most points identify the hubs as their NN. These hubs are usually near the means (Ex: dull gray images, sky and clouds). To avoid having everything classified as these hubs, we can use cosine similarity.\\
{\bf K-d trees} increase the efficiency of nearest neighbor lookup.

%Locality Sensitive Hashing

\subsection*{Gradients}
\(\frac{\partial \bf{y}}{\partial \bf{x}}\triangleq
    \begin{bmatrix}
    \frac{\partial y_1}{\partial x_1} & \dots & \frac{\partial y_m}{\partial x_1} \\
    \vdots & \ddots & \vdots\\
    \frac{\partial y_1}{\partial x_n} & \dots & \frac{\partial y_m}{\partial x_n}
    \end{bmatrix},
\)
\(\frac{\partial (A{\bf x})}{\partial {\bf x}}=A^T, \frac{\partial ({\bf x}^TA)}{\partial {\bf x}}=A,\)\\
\( \frac{\partial ({\bf x}^T{\bf x})}{\partial {\bf x}}=2{\bf x}, \frac{\partial ({\bf x}^TA{\bf x})}{\partial {\bf x}}=(A+A^T){\bf x}, \frac{\partial (trBA)}{\partial A}=B^T\)

\subsection*{Generative vs. Discriminative Model}
{\bf Generative}: Model class conditional density $p(x|y)$ and find $p(y|x) \propto p(x|y)p(y)$ or model joint density $p(x,y)$ and marginalize to find $p(y=k|x) = \int_x p(x,y=k)dx$ \\
{\bf Discriminative}: Model conditional $p(y|x)$.
\newpage
\subsection{Decision Trees}
Given a set of points and classes $\{x_i,y_i\}_{i=1}^n$, test features $x_j$ and branch on the feature which ``best'' separates the data. Recursively split on the new subset of data. Growing the tree to max depth tends to overfit (training data gets cut quickly $\implies$ subtrees train on small sets). Mistakes high up in the tree propagate to corresponding subtrees. To reduce overfitting, we can prune using a validation set, and we can limit the depth.\\
DT's are prone to label noise. Building the correct tree is hard.\\
{\bf Heurisitic}: For \underline{classification}, maximize information gain \[ \max_j \ \ \operatorname{H}(D) \ - \sum_{x_j \in X_j} P(X_j = x_j) \cdot \operatorname{H}(D | X_j = x_j) \]
where $\operatorname{H}(D) = -\sum_{c \in C} P(y = c) \log[p(y = c)] $ is the entropy of the data set, $C$ is the set of classes each data point can take, and $P(y=c)$ is the fraction of data points with class $c$.\\
For \underline{regression}, minimize the variance. Same optimiation problem as above, except H is replaced with var. Pure leaves correspond to low variance, and the result is the mean of the current leaf. 

%Gini Impurity
%Misclassification Impurity

\subsection{Random Forests}
{\bf Problem}: DT's are \underline{unstable}: small changes in the input data have large effect on tree structure $\implies$ DT's are high-variance estimators.\\
{\bf Solution}: Random Forests train $M$ different trees with randomly sampled subsets of the data (called bagging), and sometimes with randomly sampled subsets of the features to decorrelate the trees. A new point is tested on all $M$ trees and we take the majority as our output class (for regression we take the average of the output).

\subsection{Boosting}
Weak Learner: Can classify with at least 50\% accuracy.\\
Train weak learner to get a weak classifier. Test it on the training data, up-weigh misclassified data, down-weigh correctly classified data. Train a new weak learner on the weighted data. Repeat. A new point is classified by every weak learner and the output class is the sign of a weighted avg. of weak learner outputs. Boosting generally overfits. If there is label noise, boosting keeps upweighing the mislabeled data.\\
{\bf AdaBoost} is a boosting algorithm. The weak learner weights are given by $\alpha_t=\frac{1}{2}\ln(\frac{1-\epsilon_t}{\epsilon_t})$ where $\epsilon_t=Pr_{D_t}(h_t(x_i)\ne y_i)$ (probability of misclassification). The weights are updated $D_{t+1}(i)=\frac{D_t(i)exp(-\alpha_ty_ih_t(x_i))}{Z_t}$ where $Z_t$ is a normalization factor.

\subsection{Clustering}
Unsupervised Learning (no labels).\\
    {\bf Hierarchical}:
    \begin{itemize}[topsep=0pt, partopsep=0pt]
        \setlength{\itemsep}{1pt}
        \setlength{\parskip}{0pt}
        \setlength{\parsep}{0pt}
        \item \underline{Agglomerative}: Start with n points, merge 2 closest clusters using some measure, such as: Single-link (closest pair), Complete-link (furthest pair), Average-link (average of all pairs), Centroid (centroid distance).\\
        Note: SL and CL are sensitive to outliers.
        %All heuristics give different hierarchies
        \item \underline{Divisive}: Start with single cluster, recursively divide clusters into 2 subclusters.
    \end{itemize}
    {\bf Partitioning}: Partition the data into a K mutually exclusive exhaustive groups (i.e. encode k=C(i)). Iteratively reallocate to minimize some loss function. Finding the correct partitions is hard.
    Use a greedy algorithm called K-means (coordinate decent). Loss function is non-convex thus we find local minima.\\
    {\bf K-means}: Choose clusters at random, calculate centroid of each cluster, reallocate objects to nearest centroid, repeat.\\
    %K-means works well if: Clusters are spherical, Clusters are well separated, Clusters are of similar volumes, Clusters have similar number of points
    {\bf K-means}++: Initialize clusters one by one. D(x) = distance of point x to nearest cluster. Pr(x is new cluster center)$\propto D(x)^2$\\
    {\bf K-medians}: Works with arbitrary distance/dissimilarty metric, the centers $\mu_k$ are represented by data points. Is more restrictive thus has higher loss.\\
    {\bf General Loss}: $\sum_{n=1}^N\sum_{k=1}^Kd(x_n, \mu_k)r_{nk}$ where $r_{nk}=1$ if $x_n$ is in cluser k, and 0 o.w.

%Online k-means: 1) classify the point, 2) slightly nudge the center in the direction of the point.

\subsection{Vector Quantization}
Use clustering to find representative prototype vectors, which are used to simplify representations of signals.

\subsection{Parametric Density Estimation}
(Mixture Models). Assume PDF is made up of multiple gaussians with different centers. $P(x)=\sum_{i=1}^{n_c}P(c_i)P(x|c_i)$ with objective function as log likelihood of data. Use EM to estimate this model. 
\\E Step: $P(\mu_i | x_k) = \frac{P(\mu_i) P(x_k | \mu_i)}{\sum_j P(\mu_j) P(x_j|\mu_j)}$
\\M Step: $P(c_i) = \frac{1}{n_e} \sum_{k=1}^{n_e} P(\mu_i | x_k)$
\\$\mu_i = \frac{\sum_k x_k P(\mu_i|x_k)}{\sum_k P(\mu_i | x_k)}$
	\\$\sigma_i^2=\frac{\sum_k (x_k-\mu_i)^2 P(\mu_i|x_k)}{\sum_k P(\mu_i | x_k)}$.
\subsection{Non-parametric Density Estimation}
Can use Histogram or Kernel Density Estimation (KDE).\\
KDE: $P(x) = \frac{1}{n} \sum K({\bf x}-{\bf x_i})$ is a function of the data.\\
The kernel K has the following properties:\\
Symmetric, Normalized $\int_{\mathbb{R}^d}K(x)dx=1$, and $\lim_{||x|| \rightarrow \infty} ||x||^d K(x) = 0$.\\
The \underline{bandwidth} is the width of the kernel function. Too small = jagged results, too large = smoothed out results.

\subsection{Mode Seeking}
To find ``Bumps" in the distribution, we can just estimate the gradient.\\
%Taking the derivative of the KDE
Mean Shift: Translate kernel window by m(x).\\
\[m(x)=\left[ \frac{\sum_{i=1}^n x_i g(\frac{||x-x_i||^2}{h})}{\sum_{i=1}^n g(\frac{||x-x_i||^2}{h})} - x \right] \]
Where g is a constant Kernel with radius R (Can use any Kernel for generalized results).\\
To deal with saddle points, perturb modes and prune by taking the highest mode.\\
\underline{Pro's}:
\begin{itemize}[topsep=0pt, partopsep=0pt]
    \setlength{\itemsep}{1pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item Auto convergence speed, near maxima the steps are small.
    \item Only one parameter to choose (window size).
    \item Does not assume prior shape on data.
\end{itemize}
\underline{Con's}:
\begin{itemize}[topsep=0pt, partopsep=0pt]
    \setlength{\itemsep}{1pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item The window size is non-trivial, and incorrect window size can cause modes to be merged (or can cause additional ``shallow" modes to be generated)
\end{itemize}

\vfill
\columnbreak
\subsection{Neural Networks}
Neural Nets explore what you can do by combining perceptrons, each of which is a simple linear classifier. We use a soft threshold for each activation function $\theta$ because it is twice differentiable.
\includegraphics[scale=0.31]{NN.pdf} \ \includegraphics[scale=0.2]{NN2.pdf}
{\bf Activation Functions:}\\[4pt]
\ \ \ \(\theta(s) = \operatorname{tanh}(s) = \frac{e^s-e^{-s}}{e^s+e^{-s}} \implies \theta'(s) = 1 - \theta^2(s)\)\\[4pt]
\ \ \ \(\theta(s) = \sigma(s) = \frac{1}{1+e^{-s}} \implies \theta'(s) = \sigma(s)(1-\sigma(s))\)\\[4pt]
{\bf Error Functions}:\\
\ \ \(\text{Cross Entropy Loss} \sum_{i=1}^{n_{out}} y \log(h_\theta(x)) + (1-y)\log(1-h_\theta(x))\)\\[5pt]
\ \ \(\text{Mean Squared Error} \sum_{i=1}^{n_{out}} (y-h_\theta(x))^2\)\\[5pt]
{\bf Notation:}
\begin{enumerate}[topsep=0pt, partopsep=0pt]
\setlength{\itemsep}{1pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\item $w_{ij}^{(l)}$ is the weight from neuron $i$ in layer $l-1$ to neuron $j$ in layer $l$. There are $d^{(l)}$ nodes in the $l^{\text{th}}$ layer.
\item $L$ layers, where L is output layer and data is 0'th layer.
\item \(x_j^{(l)} = \theta(s_j^{(l)})\) is the output of a neuron. It's the activation function applied to the input signal. \(s_j^{(l)}=\sum_i w_{ij}^{(l)}x_i^{(l-1)}\)
\item $e(w)$ is the error as a function of the weights
\end{enumerate}
\underline{The goal is to learn the weights $w_{ij}^{(l)}$}. We use gradient descent, but error function is non-convex so we tend to local minima. The naive version takes $O(w^2)$. \underline{Back propagation}, an algorithm for efficient computation of the gradient, takes $O(w)$.\\
\[\nabla e(w) \rightarrow \frac{\partial e(w)}{\partial w_{ij}^{(l)}} = \frac{\partial e(w)}{\partial s_{j}^{(l)}} \frac{\partial s_{j}^{(l)}}{\partial w_{ij}^{(l)}} = \delta_j^{(l)}x_i^{(l-1)}\]
Final Layer: \(\delta_j^{(L)} = \frac{\partial e(w)}{\partial s_{j}^{(l)}} = \frac{\partial e(w)}{\partial x_j^{(L)}} \frac{\partial x_j^{(L)}}{\partial s_j^{(L)}} = e'(x_j^{(L)}) \theta_{out}'(s_j^{L})\) \\
\begin{eqnarray*} \text{General: } \delta_i^{(l-1)} = \frac{\partial e(w)}{\partial s_{i}^{(l-1)}} &=& \sum_{j=1}^{d^{(l)}} \frac{\partial e(w)}{\partial s_{j}^{(l)}} \times \frac{\partial s_{j}^{(l)}}{\partial x_i^{(l-1)}} \times \frac{\partial x_i^{(l-1)}}{\partial s_i^{(l-1)}} \\
&=&  \sum_{j=1}^{d^{(l)}} \delta_j^{(l)} \times w_{ij}^{(l)} \times \theta'(s_i^{(l-1)}) \end{eqnarray*}
\includegraphics[scale=0.28]{NN1.pdf}

%$W^{(l)} = W^{(l)} - \eta x^{(l-1)}(\delta^{(l)})^T$\\
%$b^{(l)} = b^{(l)} + \eta\delta^{(l)}$
%add vectorized delta for output layer and hidden layer
%$\delta^{(l-1)}=W^{(l)}\delta^{(l)}\odot\theta'(s^{(l-1)})$
%$\delta^{(L)}=e(x^{(L)})\odot\theta'(s^{(L)})$

%PCA, SVD

% You can even have references
\rule{0.3\linewidth}{0.25pt}
\newpage
\scriptsize
\bibliographystyle{abstract}
\bibliography{refFile}
\end{multicols}
\end{document}
